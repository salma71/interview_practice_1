# Terms difinitions
-----


| term | definitions | Ref
| ------ | ------ | ------
| Heteroscedasticity | Refers to a condition in which the variance of the residual term, or error term, in a regression model varies widely.  | [Heteroscedasticity](https://statisticsbyjim.com/regression/heteroscedasticity-regression/) and [here](https://www.investopedia.com/terms/h/heteroskedasticity.asp)
| Homoscedasticity | indicates that a DV's variability is equal across values of an IV. | [Heteroscedasticity](https://statisticsbyjim.com/regression/heteroscedasticity-regression/)
| Why SVM is linear? | Because it creates a line (in general, a hyperplane - boundary) that separates the data into classes, the variation in the boundary width before hitting a data point is called margin. Support vectors are those data points that the boundary pushes up against (the maximum boundary classifier is called a Support Vector Machine(in this case, a linear SVM). | [Reference](https://courses.cs.washington.edu/courses/cse473/13au/slides/26-SVMs.pdf)
| why SVM cannot be used to classify more than 2 classes? | The way SVM is defined that it only applies to classify two classes by creating a separating boundary. Since we have four classes to classify, the data is clearly not linearly separable. We cannot draw a straight line to classify the data or convert this data to linearly data in a higher dimension. Thus, the SVM algorithm won't be applicable. |  
| SVM requires a lot of data to train and it is computationally expensive. The derivations are not necessarily easy to grasp. However, SVM is one of the easiest classifiers to explain and fastest in scoring once the model is trained! Explain why? | According to prof. Patrick's lecture, in SVM, the optimization depends only on the dot product of pairs of samples 36:53. SVM is defined by a convex space; this means that it can never get stuck in a local maximum 39:48. That leads us to produce a global mechanism that easily allows us to transform into another space like a charm 45:37. | [lec_16, MIT_OCW](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/lecture-16-learning-support-vector-machines/)
| To combat over-fitting, penalization is often used. There are two kinds of penalization: Lasso (L-1) and Ridge (L-2). Ridge Regularization is function of quadratic in betas and Lasso is linear in betas. Consquently, Lasso can also be used as a tool for feature selection. If your Chief DataScience Officer asks you to reduce the complexity of the model, using penalization, will you choose Lasso or Ridge regularization? | LASSO is a good choice; it reduces the number of Instrumental Variables since we do not estimate their effects. | 
| Is it possible to penalize kNN? Explain? | No. During testing, KNN is supposed to find the closest example in the training set and return the corresponding label; it finds the K nearest neighbors and returns the majority vote of their labels. There are many approaches to selecting the best model; we can estimate each model's performance based on the validation set. This would estimate of the generalization error. Also, we could use k-fold cross validation if the data is small. | [Reference](https://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall07/L4_knn.pdf) 
| ogistic Regression, NB, LDA/QDA, kNN, Tree, SVM are the ONLY approved classifers, in your company. If robustness to outliers is the ONLY criteria you are allowed to use to select a classifier, which one would you choose? | I would choose the tree classifier. | [Reference](https://www.researchgate.net/publication/303409543_On_the_Robustness_of_Decision_Tree_Learning_under_Label_Noise)

